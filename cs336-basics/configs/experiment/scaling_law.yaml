# @package _global_

model:
  vocab_size: 50257
  context_length: 1024
  d_model: 384
  d_ff: 1024
  num_layers: 5
  num_heads: 6
  rope_theta: 10000.0

paths:
  train_bin: /workspace/dataset/train
  valid_bin: /workspace/dataset/val
  model_output: output/scaling_law

training:
  wandb_entity: yuda  # Set to your wandb entity, e.g. pliang
  wandb_project: pretrain
  train_batch_size: 64
  eval_batch_size: 64
  train_steps: 2000
  gradient_accumulation_steps: 4
  eval_iterations: null
  eval_interval: 50000
  lr: 1.5e-3
  embeddings_scale: 12.0
  init_std: 2e-2
